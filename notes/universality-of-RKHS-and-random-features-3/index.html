<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.16.4 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>            Universal Approximation Property of RKHS and Random Features (3) - 孙轶同的博客      Yitong’s Blog      </title>
<meta name="description" content="We have seen the universal approximation property of RKHS generated by radial kernels and of one-hidden-layer neural networks with sigmoidal activation functions, in previous notes. However, they only confirm that the function class considered can approximate continuous functions defined on a compact space, and hence all  functions, as well as we want. We do not know that how many samples we need, in the case of kernel method, or how many nodes we need, in the case of neural nets. To understand this, we need a more quantitative characterization of the approximation property.">



<meta property="og:type" content="article">
<meta property="og:locale" content="en">
<meta property="og:site_name" content="孙轶同的博客|Yitong's Blog">
<meta property="og:title" content="Universal Approximation Property of RKHS and Random Features (3)">
<meta property="og:url" content="https://blog.syitong.me/notes/universality-of-RKHS-and-random-features-3/">


  <meta property="og:description" content="We have seen the universal approximation property of RKHS generated by radial kernels and of one-hidden-layer neural networks with sigmoidal activation functions, in previous notes. However, they only confirm that the function class considered can approximate continuous functions defined on a compact space, and hence all  functions, as well as we want. We do not know that how many samples we need, in the case of kernel method, or how many nodes we need, in the case of neural nets. To understand this, we need a more quantitative characterization of the approximation property.">







  <meta property="article:published_time" content="2018-03-06T00:00:00-05:00">





  

  


<link rel="canonical" href="https://blog.syitong.me/notes/universality-of-RKHS-and-random-features-3/">







  <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Person",
      "name": "Yitong Sun",
      "url": "https://blog.syitong.me",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="https://blog.syitong.me/feed.xml" type="application/atom+xml" rel="alternate" title="孙轶同的博客|Yitong's Blog Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- Mathjax support -->

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


<!-- For all browsers -->
<link rel="stylesheet" href="https://blog.syitong.me/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->

<meta http-equiv="cleartype" content="on">

<!-- Baidu Autopush -->
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>

<!-- Baidu header-->


    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single wide">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">孙轶同的博客|Yitong's Blog</a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/categories/" >Categories</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/" >Tags</a>
            </li><li class="masthead__menu-item">
              <a href="/years/" >Years</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle Menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      

      
        <img src="/assets/images/icon128.png" alt="Yitong Sun" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Yitong Sun</h3>
    
    
      <p class="author__bio" itemprop="description">
        长风破浪会有时
      </p>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Universal Approximation Property of RKHS and Random Features (3)">
    <meta itemprop="description" content="We have seen the universal approximation property of RKHS generated by radial kernels and of one-hidden-layer neural networks with sigmoidal activation functions, in previous notes. However, they only confirm that the function class considered can approximate continuous functions defined on a compact space, and hence all  functions, as well as we want. We do not know that how many samples we need, in the case of kernel method, or how many nodes we need, in the case of neural nets. To understand this, we need a more quantitative characterization of the approximation property.">
    <meta itemprop="datePublished" content="March 06, 2018">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Universal Approximation Property of RKHS and Random Features (3)
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  2 minute read
</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <p>We have seen the universal approximation property of RKHS generated by radial kernels and of one-hidden-layer neural networks with sigmoidal activation functions, in previous notes. However, they only confirm that the function class considered can approximate continuous functions defined on a compact space, and hence all <script type="math/tex">L^p</script> functions, as well as we want. We do not know that how many samples we need, in the case of kernel method, or how many nodes we need, in the case of neural nets. To understand this, we need a more quantitative characterization of the approximation property.</p>

<p><a class="citation" href="#Barron1993">(Barron, 1993)</a> provides us such a characterization. It says that for any function <script type="math/tex">f</script> in a class <script type="math/tex">\Gamma_C</script>, we can find a one-hidden-layer neural network with <script type="math/tex">n</script> nodes such that their <script type="math/tex">L^2</script> distance is less than <script type="math/tex">C/\sqrt{n}</script>. <script type="math/tex">\Gamma_C</script> is defined using the first moment of the frequency distribution of <script type="math/tex">f</script>,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
  \Gamma_C := \left\{ f(x)=f(0)+\int \left(e^{i\omega\cdot x}-1\right)~\mathrm{d}\rho(\omega) : \int \vert\omega\vert_\mathcal{X}~\vert\mathrm{d}\rho(\omega)\vert < C \right\}\,,
\end{equation} %]]></script>

<p>where <script type="math/tex">\rho</script> is a complex measure on <script type="math/tex">\mathbb{R}^d</script> and <script type="math/tex">\vert\omega\vert_\mathcal{X}=\sup_{x\in\mathcal{X}}\vert\omega\cdot x\vert</script>. It is easy to see that when the first moment condition is satisfied, the integral <script type="math/tex">\int (e^{i\omega\cdot x}-1)~\mathrm{d}\rho(\omega)</script> is well defined.</p>

<p>The flavor of the result and its proof is very similar with the following theorem, which is used in <a class="citation" href="#Rahimi2009">(Rahimi &amp; Recht, 2009)</a> to prove the approximation property of random features method.</p>

<p><strong>Theorem.</strong> For <script type="math/tex">n</script> i.i.d. random variables <script type="math/tex">X_1,\ldots,X_n</script> with values with the unit ball of a Hilbert space, with probability greater than <script type="math/tex">1-\delta</script>, we have</p>

<script type="math/tex; mode=display">\begin{equation}
  \left\Vert \frac{1}{n}\sum_{i=1}^n X_i - \mathbb{E}X_1 \right\Vert \le \frac{1}{\sqrt{n}}\left(1+\sqrt{2\log\frac{1}{\delta}}\right)\,.
\end{equation}</script>

<p>This is a general high-probability result, but we can easily get an existence result by sending <script type="math/tex">\delta</script> to 1 and end up with <script type="math/tex">1/\sqrt{n}</script> as the upper bound. One can expect that the Hilbert space in theorem is actually the <script type="math/tex">L^2</script> space with respect to some probability measure <script type="math/tex">\mu</script> over the compact space <script type="math/tex">\mathcal{X}</script> in Barron’s result. To fully understand the relation between this theorem and Barron’s result, we only need to figure out how to construct <script type="math/tex">X_i</script>s from the sigmoidal functions <script type="math/tex">\sigma</script> and what is the constraint on <script type="math/tex">\mathbb{E}X_1</script>.</p>

<p><a class="citation" href="#Barron1993">(Barron, 1993)</a> shows that the functions in <script type="math/tex">\Gamma_C</script> are in the closure of the convex hull of the set of functions</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
  G_\sigma:=\left\{ \gamma\sigma(w\cdot x+b): \vert\gamma\vert<2C \right\}\,.
\end{equation} %]]></script>

<p>The proof runs as follows. First, by the Fourier form of the function, we know that for <script type="math/tex">\omega\ne 0</script>,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
  f(x) - f(0) & = \int \cos(\omega\cdot x+b(\omega))-\cos(b(\omega))~\mathrm{d}\vert\rho(\omega)\vert \\
  & = \int \frac{C_f}{\vert\omega\vert_\mathcal{X}}\left(\cos(\omega\cdot x+b(\omega))-\cos(b(\omega))\right)~\frac{\vert\omega\vert_\mathcal{X}}{C_f}\vert\mathrm{d}\rho(\omega)\vert\,.
\end{align} %]]></script>

<p><script type="math/tex">C_f=\int\vert\omega\vert_\mathcal{X}~\vert\mathrm{d}\rho\vert \le C</script> and thus, <script type="math/tex">\vert\omega\vert_\mathcal{X}\vert\mathrm{d}\rho\vert/C_f</script> is a probability measure, denoted by <script type="math/tex">\mathrm{d}\Lambda</script>. So we get that <script type="math/tex">f(x)-f(0)</script> belongs to the closure of the convex combination of the set of functions</p>

<script type="math/tex; mode=display">\begin{equation}
  G_{\cos} := \left\{ \frac{\gamma}{\vert\omega\vert_\mathcal{X}}\cos(\omega\cdot x+b(\omega))-\cos(b(\omega)): \vert\gamma\vert\le C, b\in\mathbb{R} \right\}
\end{equation}</script>

<p>Then, we further decompose <script type="math/tex">g(\omega,b)\in G_{\cos}</script> into two functions,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
  g(z) & =\frac{\gamma}{\vert\omega\vert_\mathcal{X}}(\cos(\vert\omega\vert_\mathcal{X}z+b)-\cos(b)) \\
  z & = \frac{\omega}{\vert\omega\vert_\mathcal{X}}\cdot x
\end{align} %]]></script>

<p>By the definition, <script type="math/tex">\vert z\vert\le 1</script>. We can approximate <script type="math/tex">g(z)</script> using step functions uniformly over <script type="math/tex">[-1,1]</script>. In particular, note that <script type="math/tex">g(z)</script> is <script type="math/tex">C</script>-Lipschitz, the total variation is always bounded by <script type="math/tex">2C</script> over <script type="math/tex">[-1,1]</script>. And since <script type="math/tex">g(0)=0</script>, for any <script type="math/tex">\sum \alpha_i\mathbb{1}_{\{z>t_i\}}</script> to approximate <script type="math/tex">g(z)</script>, we have <script type="math/tex">\sum\vert\alpha_i\vert\le 2C</script>. So we get that <script type="math/tex">f(x)-f(0)</script> belongs to the closure of the convex combination of the set of functions</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
  G_{\text{step}}:=\left\{ \gamma\mathbf{1}_{\{\alpha\cdot x+b>0\}}:\vert\gamma\vert<2C,\vert\alpha\vert_\mathcal{X}=1,\vert b\vert\le 1 \right\}\,.
\end{equation} %]]></script>

<p>Now we want to close the argument by showing that the step functions in <script type="math/tex">G_\text{step}</script> can be approximated by <script type="math/tex">G_\sigma</script>. To see this we only need to consider the sequence <script type="math/tex">\sigma_n = (s_n^2(\alpha\cdot x+b)-s_n)</script>, where <script type="math/tex">s_n\to\infty</script>. Then</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
  \sigma_n\to
  \begin{cases}
    1\quad & \alpha\cdot x + b > 0\,; \\
    0\quad & \alpha\cdot x + b \le 0\,,
  \end{cases}
\end{equation} %]]></script>

<p>pointwise. Then by dominant convergence theorem, this convergence also holds in <script type="math/tex">L^2(\mathcal{X},\mu)</script>. Therefore, we finally show that <script type="math/tex">f(x)-f(0)</script> belongs to the closure of the convex hull of <script type="math/tex">G_\sigma</script>.</p>
<h2 id="reference">Reference</h2>
<ol class="bibliography"><li><span id="Barron1993">Barron, A. R. (1993). Universal Approximation Bounds for Superpositions of a Sigmoidal Function. <i>IEEE Transactions on Information Theory</i>, <i>39</i>, 930–945.</span></li>
<li><span id="Rahimi2009">Rahimi, A., &amp; Recht, B. (2009). Weighted Sums of Random Kitchen Sinks: Replacing Minimization with Randomization in Learning. In D. Koller, D. Schuurmans, Y. Bengio, &amp; L. Bottou (Eds.), <i>Advances in Neural Information Processing Systems 21</i> (pp. 1313–1320). Curran Associates, Inc.</span></li></ol>

        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/tags/statistical-learning" class="page__taxonomy-item" rel="tag">statistical learning</a>
    
    </span>
  </p>




  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/categories/notes" class="page__taxonomy-item" rel="tag">notes</a>
    
    </span>
  </p>


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2018-03-06T00:00:00-05:00">March 06, 2018</time></p>
        
      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=Universal+Approximation+Property+of+RKHS+and+Random+Features+%283%29%20https%3A%2F%2Fblog.syitong.me%2Fnotes%2Funiversality-of-RKHS-and-random-features-3%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fblog.syitong.me%2Fnotes%2Funiversality-of-RKHS-and-random-features-3%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fblog.syitong.me%2Fnotes%2Funiversality-of-RKHS-and-random-features-3%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/notes/universality-of-RKHS-and-random-features-2/" class="pagination--pager" title="Universal Approximation Property of RKHS and Random Features (2)
">Previous</a>
    
    
      <a href="/notes/bessel/" class="pagination--pager" title="Bessel Functions and Fourier Transform of Radial Functions
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You May Also Enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/notes/sort-select/" rel="permalink">快速排序与快速选择算法
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  less than 1 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">之所以突然会对这个问题感兴趣是因为，大概一年前，在毫无准备的情况下去参加某互联网公司的面试，被问到了这样一个问题：“给定一个长度为n的数列，如何快速的找出其中第m大的元素。假设m远小于n。”因为对排序和选择算法完全不熟悉，只知道quicksort的时间复杂度应该是，以及从数列中找出最大值的复杂度是 。只好回答最简...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/articles/rff-intro/" rel="permalink">What Is Random Fourier Features Method?
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  2 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Random Fourier features method, or more general random features method is a method to help transform data which are not linearly separable to linearly separa...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/articles/tmux-rename/" rel="permalink">如何阻止ssh重命名tmux窗口
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  less than 1 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">在使用tmux多窗口终端时，每次登录学校的服务器后，窗口的标签就会被改成与服务器的prompt相同。而且登出后也不会改回来，导致tmux经常几个窗口的名字都很长，也没有反映窗口当时的状况。之所以会这样，是因为tmux默认允许一些进程修改窗口名，而ssh对终端窗口的命名规则是由服务器上的配置文件决定的。

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/articles/cb-setup/" rel="permalink">HiDPI Chromebook上Crouton的设置
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  less than 1 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">入手HP Chromebook 13 g1大半年，一开始就安装了Gallium OS。但Gallium OS迟迟无法解决intel skylake model上的音频输出问题，加上Gallium OS的电源管理比Chrome OS要弱不少，不接电源的情况下无法长时间的使用。忍了大半年后，终于回到了Crouton的...</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2019 Yitong Sun. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script defer src="https://use.fontawesome.com/releases/v5.8.2/js/all.js" integrity="sha384-DJ25uNYET2XCl5ZF++U8eNxPWqcKohUUBUpKGlNLMchM7q4Wjg2CUpjHLaL8yYPH" crossorigin="anonymous"></script>










  </body>
</html>
