<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.16.4 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>            Universal Approximation Property of RKHS and Random Features (2) - 孙轶同的博客      Yitong’s Blog      </title>
<meta name="description" content="Universal Approximation Property of RKHSIn this note, we discuss the universal approximation property of RKHS and compare with the property of neural network. The material is mainly based on (Micchelli, Xu, &amp; Zhang, 2006) and (Cybenko, 1989).">



<meta property="og:type" content="article">
<meta property="og:locale" content="en">
<meta property="og:site_name" content="孙轶同的博客|Yitong's Blog">
<meta property="og:title" content="Universal Approximation Property of RKHS and Random Features (2)">
<meta property="og:url" content="https://syitong.github.io/notes/universality-of-RKHS-and-random-features-2/">


  <meta property="og:description" content="Universal Approximation Property of RKHSIn this note, we discuss the universal approximation property of RKHS and compare with the property of neural network. The material is mainly based on (Micchelli, Xu, &amp; Zhang, 2006) and (Cybenko, 1989).">







  <meta property="article:published_time" content="2018-03-01T00:00:00-05:00">





  

  


<link rel="canonical" href="https://syitong.github.io/notes/universality-of-RKHS-and-random-features-2/">







  <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Person",
      "name": "Yitong Sun",
      "url": "https://syitong.github.io",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="https://syitong.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="孙轶同的博客|Yitong's Blog Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- Mathjax support -->

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


<!-- For all browsers -->
<link rel="stylesheet" href="https://syitong.github.io/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->

<meta http-equiv="cleartype" content="on">

<!-- Baidu Autopush -->
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>

<!-- Baidu header-->


    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single wide">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">孙轶同的博客|Yitong's Blog</a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/categories/" >Categories</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/" >Tags</a>
            </li><li class="masthead__menu-item">
              <a href="/years/" >Years</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle Menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      

      
        <img src="/assets/images/icon128.png" alt="Yitong Sun" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Yitong Sun</h3>
    
    
      <p class="author__bio" itemprop="description">
        长风破浪会有时
      </p>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Universal Approximation Property of RKHS and Random Features (2)">
    <meta itemprop="description" content="Universal Approximation Property of RKHSIn this note, we discuss the universal approximation property of RKHS and compare with the property of neural network. The material is mainly based on (Micchelli, Xu, &amp; Zhang, 2006) and (Cybenko, 1989).">
    <meta itemprop="datePublished" content="March 01, 2018">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Universal Approximation Property of RKHS and Random Features (2)
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  3 minute read
</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <h2 id="universal-approximation-property-of-rkhs">Universal Approximation Property of RKHS</h2>
<p>In this note, we discuss the universal approximation property of RKHS and compare with the property of neural network. The material is mainly based on <a class="citation" href="#Micchelli2006">(Micchelli, Xu, &amp; Zhang, 2006)</a> and <a class="citation" href="#Cybenko1989">(Cybenko, 1989)</a>.</p>

<p>The universal approximation property says that the hypothesis class accessible by the learning model is dense in some common function class. For continuous kernels, its hypothesis class is a subset of continuous functions, and thus we naturally consider whether it can approximates <script type="math/tex">C(\mathcal{X})</script>, where <script type="math/tex">\mathcal{X}</script> is a compact subset of <script type="math/tex">\mathbb{R}^d</script> under the sup norm. In functional analysis, to prove that the subspace spanned by a subset is dense in a space, we only need to show that its only annihilator is <script type="math/tex">0</script>, which is a consequence of Hahn-Banach theorem.</p>

<p>The dual space of <script type="math/tex">C(\mathcal{X})</script> consists of all the complex valued Radon measures on <script type="math/tex">\mathcal{X}</script>, denoted by <script type="math/tex">B(\mathcal{X})</script>. For a RKHS generated by a continuous kernel <script type="math/tex">k</script>, assume that <script type="math/tex">W</script> is its feature space and <script type="math/tex">\phi:\mathcal{X}\to W</script> its feature map. We can define a map <script type="math/tex">U</script> from <script type="math/tex">B(\mathcal{X})</script> to <script type="math/tex">W</script> to be the corresponding element in <script type="math/tex">W</script> such that</p>

<script type="math/tex; mode=display">\begin{equation}
  \langle U(\nu), w\rangle = \int_\mathcal{X}\langle \phi(x),w\rangle~\mathrm{d}\nu\,.
\end{equation}</script>

<p>Note that <script type="math/tex">\langle\phi(x),w\rangle</script> is a continuous function in the RKHS of <script type="math/tex">k</script>, so the map <script type="math/tex">U</script> is well-defined. We can denote the image of <script type="math/tex">\nu</script> by <script type="math/tex">\int\phi(x)~\mathrm{d}\nu</script> and the switch between integral and inner product is justified. And it follows this definition immediately that <script type="math/tex">\mathcal{H}_k^\bot = \ker(U)</script>. For each kernel, its RKHS is unique, but its features spaces are not. That lends us more tools to study the annihilator of <script type="math/tex">\mathcal{H}_k</script>.</p>

<p>For example, when <script type="math/tex">k</script> is the translation invariant kernel, we can apply Bochner’s theorem to construct its features space as <script type="math/tex">L^2(\mathbb{R}^d,\mathbb{P})</script>, where <script type="math/tex">\mathbb{P}</script> is a finite Borel measure, and the feature map is <script type="math/tex">\phi(x)=e^{ix\cdot w}</script>. So the question is converted to: if</p>

<script type="math/tex; mode=display">\begin{equation}
  \int_\mathcal{X}e^{i x\cdot w}~\mathrm{d}\nu = 0\quad\forall w\in\mathrm{supp}(\mathbb{P})\,,\tag{*}
\end{equation}</script>

<p>does <script type="math/tex">\nu</script> have to be <script type="math/tex">0</script>? Obviously, this question is related with the support of <script type="math/tex">\mathbb{P}</script>. If the support of <script type="math/tex">\mathbb{P}</script> is <script type="math/tex">\mathbb{R}^d</script>, then (*) implies that <script type="math/tex">\nu</script> has to be <script type="math/tex">0</script> measure. This is the case of Gaussian kernel. However, if the support of <script type="math/tex">\mathbb{P}</script> is just a single point, (*) does not imply that <script type="math/tex">\nu=0</script>. This question is related with the <a href="https://en.wikipedia.org/wiki/Set_of_uniqueness">set of uniqueness</a> in harmonic analysis.</p>

<p>For radial basis kernel of the form <script type="math/tex">g(\Vert x-y \Vert^2)</script>, it can always written into the form</p>

<script type="math/tex; mode=display">\begin{equation}
  g(\Vert x-y \Vert^2):=\int_{\mathbb{R}^+} e^{-\sigma\Vert x-y \Vert^2}~\mathrm{d}\mu(\sigma)\,,
\end{equation}</script>

<p>where <script type="math/tex">\mu</script> is a finite Borel measure on <script type="math/tex">\mathbb{R}^d</script>. Using multinomial Taylor expansion, we can construct the feature map</p>

<script type="math/tex; mode=display">\begin{equation}
  \phi(x):=\sigma^{\vert\alpha\vert/2}x^\alpha e^{-\sigma\Vert x\Vert^2}\,,
\end{equation}</script>

<p>with inner product defined by</p>

<script type="math/tex; mode=display">\begin{equation}
  \langle \phi(x),\phi(x')\rangle = \sum_{\alpha\in\mathbb{Z}^d_+}
  \begin{pmatrix}
    \vert \alpha \vert \\ \alpha
  \end{pmatrix}
  \frac{2^{\vert\alpha\vert}}{\vert\alpha\vert!}\int_{\mathbb{R}^+}\phi(x)\phi(x')~\mathrm{d}\mu(\sigma)\,.
\end{equation}</script>

<p>For such a feature map, whenever <script type="math/tex">\mathrm{supp}(\mu)</script> contains any positive number <script type="math/tex">\rho</script>, the map <script type="math/tex">U</script> is injective. Indeed, <script type="math/tex">\rho^{\vert\alpha\vert/2}x^\alpha e^{-\rho\Vert x\Vert^2}</script> of all <script type="math/tex">\alpha</script> comprise all the polynomials over <script type="math/tex">\mathcal{X}</script>, which is dense in <script type="math/tex">C(\mathcal{X})</script> and implies that <script type="math/tex">\nu</script> must be <script type="math/tex">0</script> if <script type="math/tex">\int\phi(x)~\mathrm{d}\nu=0</script>.</p>

<h2 id="universal-approximation-property-of-neural-networks">Universal Approximation Property of Neural Networks</h2>
<p>Now we consider the universal approximation property of neural networks. One hidden layer neural network has the form</p>

<script type="math/tex; mode=display">\begin{equation}
  f(x) = \sum_{i=1}^N w_{2i}\sigma(w_{1i:}\cdot x+b_{1i}) + b_2\,.
\end{equation}</script>

<p>The first subscript of the coefficients indicates the layer, the second and the third indicates its node and the third for the coordinate. The parameters <script type="math/tex">w,b</script> within layer <script type="math/tex">i</script> forms a matrix of shape <script type="math/tex">N_i\times (N_{i+1}+1)</script>, where <script type="math/tex">N_j</script> represents the number of nodes in layer <script type="math/tex">j</script>. We denote the linear span of the set <script type="math/tex">\{ \sigma(w\cdot x+b),1 \}</script> by <script type="math/tex">\mathcal{H}_\sigma</script>. To show that it is dense in <script type="math/tex">C(\mathcal{X})</script>, we also consider its annihilator. If a signed/complex measure <script type="math/tex">\nu</script> on <script type="math/tex">\mathcal{X}</script> satisfies that</p>

<script type="math/tex; mode=display">\begin{equation}
  \int_\mathcal{X} \sigma(w\cdot x+b)~\mathrm{d}\nu=0\,,
\end{equation}</script>

<p>for all <script type="math/tex">(w,b)</script> in <script type="math/tex">\mathbb{R}^{d+1}</script>, we want to show that it must be constantly <script type="math/tex">0</script> everywhere. Here we can see that it actually plays the role of the canonical feature maps in the kernel method. To show that <script type="math/tex">\nu</script> has to be constantly <script type="math/tex">0</script>, we note that for any <script type="math/tex">x,w,b</script>,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
  \sigma(s(w\cdot x+b)+t)\to
  \begin{cases}
    \lim_{u\to\infty}\sigma(u) & w\cdot x+b>0\,; \\
    \sigma(t) & w\cdot x+b=0\,; \\
    \lim_{u\to -\infty}\sigma(u) & w\cdot x+b<0\,.
  \end{cases}
\end{equation} %]]></script>

<p>If <script type="math/tex">\sigma</script> is bounded and sigmoidal, that is, <script type="math/tex">\lim_{u\to\infty}\sigma(u)=1</script> and <script type="math/tex">\lim_{u\to-\infty}\sigma(u)=0</script>, then by dominant convergence theorem, we can show that</p>

<script type="math/tex; mode=display">\begin{equation}
  \nu(w\cdot x+b>0) + \sigma(t)\nu(w\cdot x+b=0) = 0\quad\forall t\in\mathbb{R}\,,
\end{equation}</script>

<p>Therefore <script type="math/tex">\nu(w\cdot x+b>0)=0</script> for all <script type="math/tex">w,b</script>. This implies that <script type="math/tex">\nu</script> must be constantly <script type="math/tex">0</script>.</p>

<p>Actually, if we assign a probability distribution <script type="math/tex">\mu</script> over the parameter space <script type="math/tex">w,b\in \mathbb{R}^{d}\times\mathbb{R}</script>, <script type="math/tex">\sigma</script> determines a feature map <script type="math/tex">\phi:\mathcal{X}\to L^2(\mu)</script>. The corresponding kernel is defined by</p>

<script type="math/tex; mode=display">\begin{equation}
  k(x,x'):=\int_{\mathbb{R}^{d+1}} \sigma(w\cdot x+b)\sigma(w\cdot x'+b)~\mathrm{d}\mu(w,b)\,.
\end{equation}</script>

<p>For the RKHS generated by <script type="math/tex">k</script> to be dense in <script type="math/tex">C(\mathcal{X})</script>, we need some requirements on <script type="math/tex">\mu</script>. An obvious sufficient condition is that <script type="math/tex">\mathrm{supp}(\mu)=\mathbb{R}^{d+1}</script>. In this case, the RKHS generated by <script type="math/tex">k</script> can be written in the form</p>

<script type="math/tex; mode=display">\begin{equation}
  \left\{ f(x)=\int_{\mathbb{R}^{d+1}} \sigma(w\cdot x+b)g(w,b)~\mathrm{d}\mu(w,b): g(w,b)\in L^2(\mu)\right\}\,.
\end{equation}</script>

<p>For any <script type="math/tex">f</script> in this RKHS, we can always use</p>

<script type="math/tex; mode=display">\begin{equation}
  \frac{1}{N}\sum_{i=1}^N g(w_{1i},b_{1i})\sigma(w_{1i:}\cdot x+b_{1i})
\end{equation}</script>

<p>to approximate it. The quality of this approximation under <script type="math/tex">L^2</script> norm is discussed in Bach’s work.</p>
<h2 id="reference">Reference</h2>
<ol class="bibliography"><li><span id="Micchelli2006">Micchelli, C. A., Xu, Y., &amp; Zhang, H. (2006). Universal Kernels. <i>Journal of Machine Learning Research</i>, <i>7</i>, 2651–2667.</span></li>
<li><span id="Cybenko1989">Cybenko, G. (1989). Approximation by Superpositions of a Sigmoidal Function. <i>Mathematics of Control, Signals and Systems</i>, <i>2</i>, 303–314.</span></li></ol>

        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/tags/statistical-learning" class="page__taxonomy-item" rel="tag">statistical learning</a>
    
    </span>
  </p>




  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/categories/notes" class="page__taxonomy-item" rel="tag">notes</a>
    
    </span>
  </p>


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2018-03-01T00:00:00-05:00">March 01, 2018</time></p>
        
      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=Universal+Approximation+Property+of+RKHS+and+Random+Features+%282%29%20https%3A%2F%2Fsyitong.github.io%2Fnotes%2Funiversality-of-RKHS-and-random-features-2%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fsyitong.github.io%2Fnotes%2Funiversality-of-RKHS-and-random-features-2%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fsyitong.github.io%2Fnotes%2Funiversality-of-RKHS-and-random-features-2%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/notes/universality-of-RKHS-and-random-features-1/" class="pagination--pager" title="Universal Approximation Property of RKHS and Random Features (1)
">Previous</a>
    
    
      <a href="/notes/universality-of-RKHS-and-random-features-3/" class="pagination--pager" title="Universal Approximation Property of RKHS and Random Features (3)
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You May Also Enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/notes/sort-select/" rel="permalink">快速排序与快速选择算法
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  less than 1 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">之所以突然会对这个问题感兴趣是因为，大概一年前，在毫无准备的情况下去参加某互联网公司的面试，被问到了这样一个问题：“给定一个长度为n的数列，如何快速的找出其中第m大的元素。假设m远小于n。”因为对排序和选择算法完全不熟悉，只知道quicksort的时间复杂度应该是，以及从数列中找出最大值的复杂度是 。只好回答最简...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/articles/rff-intro/" rel="permalink">What Is Random Fourier Features Method?
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  2 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Random Fourier features method, or more general random features method is a method to help transform data which are not linearly separable to linearly separa...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/articles/tmux-rename/" rel="permalink">如何阻止ssh重命名tmux窗口
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  less than 1 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">在使用tmux多窗口终端时，每次登录学校的服务器后，窗口的标签就会被改成与服务器的prompt相同。而且登出后也不会改回来，导致tmux经常几个窗口的名字都很长，也没有反映窗口当时的状况。之所以会这样，是因为tmux默认允许一些进程修改窗口名，而ssh对终端窗口的命名规则是由服务器上的配置文件决定的。

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/articles/cb-setup/" rel="permalink">HiDPI Chromebook上Crouton的设置
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  less than 1 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">入手HP Chromebook 13 g1大半年，一开始就安装了Gallium OS。但Gallium OS迟迟无法解决intel skylake model上的音频输出问题，加上Gallium OS的电源管理比Chrome OS要弱不少，不接电源的情况下无法长时间的使用。忍了大半年后，终于回到了Crouton的...</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2019 Yitong Sun. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script defer src="https://use.fontawesome.com/releases/v5.8.2/js/all.js" integrity="sha384-DJ25uNYET2XCl5ZF++U8eNxPWqcKohUUBUpKGlNLMchM7q4Wjg2CUpjHLaL8yYPH" crossorigin="anonymous"></script>










  </body>
</html>
